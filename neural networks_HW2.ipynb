# Task 2: Реализовать адаптивний оптимизатор с подстраивающимся LR
import numpy as np

def f(x):
    return (x ** 2 - 5 * x + 4) ** 2

def grad_f(x):
    return 2 * (x ** 2 - 5 * x + 4) * (2 * x - 5)

def adaptive_gradient_descent(starting_point, initial_lr, num_iterations):
    x = starting_point
    lr = initial_lr
    epsilon = 1e-8
    grad_cache = 0

    for i in range(num_iterations):
        grad = grad_f(x)
        
        # Ключевое отличие: адаптируем lr на каждом шаге
        grad_cache += grad ** 2
        adapted_lr = lr / (np.sqrt(grad_cache) + epsilon)
        
        x = x - adapted_lr * grad
        print(f"Iteration {i+1}: x = {x}, f(x) = {f(x)}, LR = {adapted_lr}")
    
    return x

# Параметры для адаптивного градиентного спуска
initial_lr = 0.1
num_iterations = 100

# Поиск первого корня с начальной точкой ближе к предполагаемому первому корню
starting_point_1 = 0
optimal_x_1 = adaptive_gradient_descent(starting_point_1, initial_lr, num_iterations)
print(f"Optimal x_1: {optimal_x_1}")

# Поиск второго корня с начальной точкой ближе к предполагаемому второму корню
starting_point_2 = 5
optimal_x_2 = adaptive_gradient_descent(starting_point_2, initial_lr, num_iterations)
print(f"Optimal x_2: {optimal_x_2}")
